# LLM Model Recommendations for Trading System

## Overview
The trading system uses LLMs to power 10+ AI agents. Model selection directly impacts:
- **Speed**: How fast agents complete analysis
- **Rate Limits**: How many requests before hitting limits
- **Cost**: Free vs paid models
- **Reliability**: System stability and timeout issues

## The Problem with Large Models

### ‚ùå Large Models (70B parameters)
- Example: `llama-3.3-70b-versatile`, `llama-3.1-70b-versatile`
- Response time: 10-60 seconds per call
- With 10 agents: 10 √ó 30 sec = **5 minutes minimum**
- **Result**: Timeouts, rate limit exhaustion, system hangs

### ‚úÖ Small/Medium Models (3-8B parameters)
- Example: `llama-3.1-8b-instant`, `llama-3.2-3b-instruct`
- Response time: 2-5 seconds per call
- With 10 agents: 10 √ó 4 sec = **40 seconds**
- **Result**: Fast, reliable, completes successfully

## Recommended Models by Use Case

### üèÜ Best Overall: Groq llama-3.1-8b-instant
```bash
LLM_PROVIDER=groq
LLM_MODEL=llama-3.1-8b-instant
```

**Why it's best**:
- ‚ö° Speed: 2-5 seconds per call (fastest)
- üéØ Quality: Excellent for trading analysis
- üí∞ Cost: Free (30 req/min)
- üîÑ Reliability: 99%+ uptime

**Ideal for**: Development, testing, and production

---

### ü•à Best Fallback: Google Gemini Flash
```bash
GOOGLE_API_KEY=your_key
```

**Why it's great**:
- ‚ö° Speed: 2-5 seconds per call
- üéØ Quality: Similar to GPT-4
- üí∞ Cost: Free (60 req/min - generous!)
- üîÑ Reliability: Excellent (Google infrastructure)

**Ideal for**: Fallback provider when Groq rate-limited

---

### ü•â Budget Option: OpenRouter Free Models
```bash
OPENROUTER_API_KEY=your_key
OPENROUTER_MODEL=meta-llama/llama-3.2-3b-instruct:free
```

**Why it works**:
- ‚ö° Speed: 5-10 seconds per call
- üéØ Quality: Good enough for most analysis
- üí∞ Cost: 100% free (no rate limits)
- üîÑ Reliability: Good

**Ideal for**: Extra burst capacity, testing

---

## Model Comparison Table

| Model | Provider | Speed | Quality | Free Limit | Cost | Recommended For |
|-------|----------|-------|---------|------------|------|-----------------|
| **llama-3.1-8b-instant** | Groq | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 30/min | Free | **Primary (Best)** |
| **gemini-flash-latest** | Google | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 60/min | Free | **Fallback** |
| **llama-3.2-3b:free** | OpenRouter | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | 50/min | Free | Extra capacity |
| llama-3.1-70b-versatile | Groq | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 30/min | Free | ‚ùå Too slow |
| llama-3.3-70b-versatile | Groq | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 30/min | Free | ‚ùå Too slow |
| gpt-4o-mini | OpenAI | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | N/A | $0.15/1M | Production (paid) |
| mixtral-8x7b-instruct | Together | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 40/min | Free | Alternative |

---

## Provider Setup Guide

### 1. Groq (Primary - Fastest) ‚ö°

**Get API Key** (2 minutes):
1. Visit: https://console.groq.com/keys
2. Sign up (email only, no credit card)
3. Click "Create API Key"
4. Copy key

**Configuration**:
```bash
GROQ_API_KEY=gsk_xxxxx
GROQ_MODEL=llama-3.1-8b-instant
```

**Rate Limits**:
- Free tier: 30 requests/minute, 14,400/day
- Response time: 2-5 seconds
- Tokens/min: 20,000

---

### 2. Google Gemini (Fallback - Most Generous) üéØ

**Get API Key** (2 minutes):
1. Visit: https://makersuite.google.com/app/apikey
2. Click "Get API Key" ‚Üí "Create API Key"
3. Copy key

**Configuration**:
```bash
GOOGLE_API_KEY=AIzaSyxxxxx
```
(Model auto-configured to `gemini-flash-latest`)

**Rate Limits**:
- Free tier: 60 requests/minute, 1,500/day
- Response time: 2-5 seconds
- Tokens/min: 1M (!!)

---

### 3. OpenRouter (Extra - Free Models) üí∞

**Get API Key** (2 minutes):
1. Visit: https://openrouter.ai/keys
2. Sign up
3. Click "Create Key"
4. Copy key

**Configuration**:
```bash
OPENROUTER_API_KEY=sk-or-v1-xxxxx
OPENROUTER_MODEL=meta-llama/llama-3.2-3b-instruct:free
```

**Rate Limits**:
- Free models: 50 requests/minute
- Response time: 5-10 seconds
- No daily limit for :free models

---

### 4. Ollama (Local - Unlimited) üè†

**Setup** (5 minutes):
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull a fast model
ollama pull llama3.2:3b

# Start server
ollama serve
```

**Configuration**:
```bash
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

**Pros**:
- ‚úÖ No rate limits
- ‚úÖ No API key needed
- ‚úÖ 100% private
- ‚úÖ Works offline

**Cons**:
- ‚ùå Slower (5-15 seconds per call)
- ‚ùå Requires local GPU/CPU
- ‚ùå Uses RAM (8GB+ recommended)

---

## Multi-Provider Strategy (Recommended)

### Configuration for Maximum Reliability

```bash
# Primary (fastest)
LLM_PROVIDER=groq
LLM_MODEL=llama-3.1-8b-instant
GROQ_API_KEY=your_groq_key

# Fallback 1 (generous limits)
GOOGLE_API_KEY=your_google_key

# Fallback 2 (extra capacity)
OPENROUTER_API_KEY=your_openrouter_key

# Fallback 3 (local, unlimited)
OLLAMA_BASE_URL=http://localhost:11434
```

**How it works**:
1. System tries Groq first (fastest)
2. If Groq rate-limited ‚Üí switches to Gemini automatically
3. If Gemini rate-limited ‚Üí switches to OpenRouter
4. If all cloud providers down ‚Üí uses local Ollama

**Total capacity**: 30 + 60 + 50 = **140 requests/minute** üöÄ

---

## Performance Benchmarks

### Single Agent Analysis Time

| Model | Time | Notes |
|-------|------|-------|
| llama-3.1-8b-instant (Groq) | 2-5s | ‚úÖ **Recommended** |
| gemini-flash-latest (Google) | 2-5s | ‚úÖ Great fallback |
| llama-3.2-3b:free (OpenRouter) | 5-10s | ‚úÖ Good backup |
| llama-3.1-70b-versatile (Groq) | 10-30s | ‚ö†Ô∏è Too slow |
| llama-3.3-70b-versatile (Groq) | 15-60s | ‚ùå Way too slow |
| llama3.2:3b (Ollama local) | 5-15s | ‚ö†Ô∏è Depends on hardware |

### Full Trading Loop (10 Agents)

| Configuration | Total Time | Status |
|---------------|------------|--------|
| 8B instant + multi-provider | 30-90s | ‚úÖ **Perfect** |
| 8B instant + Groq only | 40-120s | ‚úÖ Good |
| 70B versatile + Groq only | 300-600s | ‚ùå Timeout risk |
| 70B versatile + multi-provider | 200-400s | ‚ö†Ô∏è Slow but works |

---

## Common Issues & Solutions

### Issue: "Analysis timed out after 5 minutes"
**Cause**: Using 70B model (too slow)
**Fix**: Change to `llama-3.1-8b-instant`

### Issue: "Rate limit exceeded"
**Cause**: Single provider, 30/min limit
**Fix**: Add Gemini fallback (60/min extra)

### Issue: "All LLM providers failed"
**Cause**: Invalid API keys or network issue
**Fix**: 
1. Verify API keys are correct
2. Check internet connection
3. Add Ollama as local fallback

### Issue: "Provider X timed out"
**Cause**: Provider API is down or slow
**Fix**: System auto-switches to next provider

---

## Cost Analysis

### Free Tier Capacity (per day)

| Provider | Free Requests | Enough For |
|----------|---------------|------------|
| Groq | 14,400/day | 720 trading cycles |
| Gemini | 1,500/day | 75 trading cycles |
| OpenRouter | Unlimited | ‚àû trading cycles |
| **Combined** | **15,900+** | **795+ cycles** |

With 60-second cycle time:
- 1 cycle/minute
- 60 cycles/hour  
- 1,440 cycles/day (24 hours)

**Free tier covers ~50% of 24/7 operation**
(More than enough for market hours: 6.5 hours = 390 cycles)

---

## Recommendations Summary

### For Development/Testing
```bash
LLM_MODEL=llama-3.1-8b-instant
GROQ_API_KEY=xxx
```
‚úÖ Fast, reliable, free

### For Production (Free)
```bash
LLM_MODEL=llama-3.1-8b-instant
GROQ_API_KEY=xxx
GOOGLE_API_KEY=xxx
OPENROUTER_API_KEY=xxx
```
‚úÖ Maximum reliability, auto-fallback, 140+ req/min

### For Production (Paid)
```bash
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=xxx
# Keep free providers as fallback
GROQ_API_KEY=xxx
GOOGLE_API_KEY=xxx
```
‚úÖ Best quality, highest reliability, ~$1-5/day

---

## Migration Guide

### From 70B to 8B Model

**Step 1**: Update `.env`
```bash
# Change this line:
LLM_MODEL=llama-3.3-70b-versatile

# To this:
LLM_MODEL=llama-3.1-8b-instant
```

**Step 2**: Restart system
```bash
python start_trading_system.py
```

**Expected changes**:
- ‚ö° 6-12x faster execution
- ‚úÖ No more timeouts
- ‚úÖ Agents complete successfully
- üìä Similar quality analysis

**Will analysis quality decrease?**
- For trading: **No significant difference**
- 8B models are excellent at structured tasks
- You might notice slightly less creative explanations
- But trading decisions remain accurate

---

## Final Recommendations

### ‚úÖ DO THIS:
1. Use `llama-3.1-8b-instant` (fast, reliable)
2. Set up Groq + Gemini fallback (140 req/min)
3. Monitor system logs for rate limits

### ‚ùå DON'T DO THIS:
1. Use 70B models (too slow, causes timeouts)
2. Rely on single provider (rate limit bottleneck)
3. Skip fallback providers (no redundancy)

### üéØ Optimal Setup:
```bash
# Primary
LLM_PROVIDER=groq
LLM_MODEL=llama-3.1-8b-instant
GROQ_API_KEY=your_key

# Fallback
GOOGLE_API_KEY=your_key
OPENROUTER_API_KEY=your_key
```

**Result**: 
- ‚ö° 30-90 second trading loops
- üîÑ 140+ requests/minute capacity
- ‚úÖ Reliable 24/7 operation
- üí∞ 100% free

üöÄ **Your agents will run smoothly!**
